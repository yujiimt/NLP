{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOAu61R5opP+l3oTr5i0QxS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yujiimt/NLP/blob/master/book/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbs8_drHwLg4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b766256a-1ca4-44c8-8de6-27cf79a34759"
      },
      "source": [
        "!mkdir data\n",
        "!wget https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/ja.text8.zip -P data/\n",
        "!unzip data/ja.text8.zip -d data/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-03 07:02:31--  https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/ja.text8.zip\n",
            "Resolving s3-ap-northeast-1.amazonaws.com (s3-ap-northeast-1.amazonaws.com)... 52.219.0.166\n",
            "Connecting to s3-ap-northeast-1.amazonaws.com (s3-ap-northeast-1.amazonaws.com)|52.219.0.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33905114 (32M) [application/zip]\n",
            "Saving to: ‘data/ja.text8.zip’\n",
            "\n",
            "ja.text8.zip        100%[===================>]  32.33M  7.73MB/s    in 4.2s    \n",
            "\n",
            "2020-05-03 07:02:36 (7.73 MB/s) - ‘data/ja.text8.zip’ saved [33905114/33905114]\n",
            "\n",
            "Archive:  data/ja.text8.zip\n",
            "  inflating: data/ja.text8           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9wAtWfxKVGv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "510ca2f6-8471-4874-fb30-8e601eafcf5c"
      },
      "source": [
        "!wget https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz -P data/\n",
        "!gunzip -d data/amazon_reviews_multilingual_JP_v1_00.tsv.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-03 07:02:57--  https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.110.5\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.110.5|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94688992 (90M) [application/x-gzip]\n",
            "Saving to: ‘data/amazon_reviews_multilingual_JP_v1_00.tsv.gz’\n",
            "\n",
            "amazon_reviews_mult 100%[===================>]  90.30M  28.7MB/s    in 3.1s    \n",
            "\n",
            "2020-05-03 07:03:00 (28.7 MB/s) - ‘data/amazon_reviews_multilingual_JP_v1_00.tsv.gz’ saved [94688992/94688992]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQxMvqKr2zg0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "7e3de5d2-fdec-49ef-c90c-4eaf831812ac"
      },
      "source": [
        "!pip install janome"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/f0/bd7f90806132d7d9d642d418bdc3e870cfdff5947254ea3cab27480983a7/Janome-0.3.10-py2.py3-none-any.whl (21.5MB)\n",
            "\u001b[K     |████████████████████████████████| 21.5MB 1.3MB/s \n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.3.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7uRYmWUiy2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm9iVlfm2i5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from bs4 import BeautifulSoup\n",
        "from janome.tokenizer import Tokenizer\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, Embedding, SimpleRNN\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "t = Tokenizer(wakati = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5DifH5Wy0Ia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_by_ascii_rate(text, threshold=0.9):\n",
        "    ascii_letters = set(string.printable)\n",
        "    rate = sum(c in ascii_letters for c in text) / len(text)\n",
        "    return rate <= threshold\n",
        "\n",
        "def load_dataset(filename, n=5000, state = 6):\n",
        "    df = pd.read_csv(filename, sep='\\t')\n",
        "\n",
        "    mapping = {1: 0, 2: 0, 4: 1, 5: 1}\n",
        "    df = df[df.star_rating != 3]\n",
        "    df.star_rating = df.star_rating.map(mapping)\n",
        "\n",
        "    is_jp = df.review_body.apply(filter_by_ascii_rate)\n",
        "    df = df[is_jp]\n",
        "\n",
        "\n",
        "    df = df.sample(frac = 1, random_state = state)\n",
        "    grouped = df.groupby('star_rating')\n",
        "    df = grouped.head(n=n)\n",
        "    return df.review_body.values, df.star_rating.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FuCJAQs2Zqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocabulary(texts, num_words=None):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words = num_words, oov_token = '<UNK>'\n",
        "    )\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def clean_html(html, strip = False):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    text = soup.get_text(strip=strip)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    return t.tokenize(text)\n",
        "\n",
        "def preprocess_dataset(texts):\n",
        "    texts = [clean_html(text) for text in texts]\n",
        "    texts = [\" \".join(tokenizer(text)) for text in texts]\n",
        "    return texts\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8E6vcDL4Ttg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModel:\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, emb_dim = 300,\n",
        "                 hid_dim = 100, embeddings = None, trainable = True):\n",
        "      self.input = Input(shape = (None, ), name = 'input')\n",
        "\n",
        "      if embeddings is None:\n",
        "        self.embedding = Embedding(input_dim = input_dim,\n",
        "                                   output_dim = emb_dim,\n",
        "                                   mask_zero = True,\n",
        "                                   trainable = trainable,\n",
        "                                   name = 'embedding')\n",
        "      else:\n",
        "          self.embedding = Embedding(input_dim = embeddings.shape[0],\n",
        "                                     output_dim = embeddings.shape[1],\n",
        "                                     mask_zero = True,\n",
        "                                     trainable = trainable,\n",
        "                                     weights = [embeddings],\n",
        "                                     name = 'embedding')\n",
        "      self.rnn = SimpleRNN(hid_dim, name = 'rnn')\n",
        "      self.fc = Dense(output_dim, activation = 'softmax')\n",
        "\n",
        "    def build(self):\n",
        "        x = self.input\n",
        "        embedding = self.embedding(x)\n",
        "        output = self.rnn(embedding)\n",
        "        y = self.fc(output)\n",
        "        return Model(inputs = x, outputs = y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT5MwUwJC255",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InferenceAPI:\n",
        "\n",
        "    def __init__(self, model, vocab, preprocess):\n",
        "        self.model = model\n",
        "        self.vocab = vocab\n",
        "        self.preprocess = preprocess\n",
        "\n",
        "\n",
        "    def predict_from_texts(self, texts):\n",
        "        x = self.preprocess(texts)\n",
        "        x = self.voca.texts.texts_to_sequences(x)\n",
        "        return self.predict_from_sequences(x)\n",
        "\n",
        "    def predict_from_sequences(self, sequences):\n",
        "        sequences = pad_sequences(sequences, truncating='post')\n",
        "        y = self.model.predict(sequences)\n",
        "        return np.argmax(y, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5dI5BoMFdDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    batch_size = 128\n",
        "    epochs = 100\n",
        "    maxlen = 300\n",
        "    model_path = 'models/rnn_models.h5'\n",
        "    num_words = 40000\n",
        "    num_label = 2\n",
        "\n",
        "    x, y = load_dataset('/content/data/amazon_reviews_multilingual_JP_v1_00.tsv')\n",
        "\n",
        "\n",
        "  # 前処理\n",
        "    x = preprocess_dataset(x)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
        "    vocab = build_vocabulary(x_train, num_words)\n",
        "    x_train = vocab.texts_to_sequences(x_train)\n",
        "    x_test = vocab.texts_to_sequences(x_test)\n",
        "    x_train = pad_sequences(x_train, maxlen = maxlen, truncating=\"post\")\n",
        "    x_test = pad_sequences(x_test, maxlen = maxlen, truncating='post')\n",
        "\n",
        "    model = RNNModel(num_words, num_label, embeddings=None).build()\n",
        "    model.compile(optimizer = 'adam',\n",
        "                loss = 'sparse_categorical_crossentropy',\n",
        "                metrics = ['acc'])\n",
        "    callbacks = [\n",
        "               EarlyStopping(patience=3),\n",
        "               ModelCheckpoint(model_path, save_best_only=True)\n",
        "  ]            \n",
        "\n",
        "    model.fit(\n",
        "      x = x_train,\n",
        "      y = y_train,\n",
        "      batch_size = batch_size,\n",
        "      epochs = epochs,\n",
        "      validation_split = 0.2,\n",
        "      callbacks = callbacks,\n",
        "      shuffle = True\n",
        "        )\n",
        "  \n",
        "    model = load_model(model_path)\n",
        "    api = InferenceAPI(model, vocab, preprocess_dataset)\n",
        "    y_pred = api.predict_from_sequences(x_test)\n",
        "    print('precision: {:.4f}'.format(precision_score(y_test, y_pred, average = 'binary')))\n",
        "    print('recall : {:.4f}'.format(recall_score(y_test, y_pred, average = 'binary')))\n",
        "    print('f1 : {:.4f}'.format(f1_score(y_test, y_pred, average = 'binary')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgtWpr95WXBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2460ecff-d85b-4810-a419-b591132721fa"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "50/50 [==============================] - 16s 319ms/step - loss: 0.6518 - acc: 0.6072 - val_loss: 0.5917 - val_acc: 0.6844\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 15s 306ms/step - loss: 0.2754 - acc: 0.8978 - val_loss: 0.6461 - val_acc: 0.6931\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 15s 308ms/step - loss: 0.0415 - acc: 0.9909 - val_loss: 0.6270 - val_acc: 0.7462\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 15s 308ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.6851 - val_acc: 0.7381\n",
            "precision: 0.6631\n",
            "recall : 0.8018\n",
            "f1 : 0.7259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZKEd6SyZQG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
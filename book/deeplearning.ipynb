{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN491O2oiunqjiF234V81us",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yujiimt/NLP/blob/master/book/deeplearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qllnrhFz-VYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Input, Dot, Flatten, Embedding, Dense\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from pprint import pprint\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm26KTGQ3pBJ",
        "colab_type": "code",
        "outputId": "f12ea3db-dc0a-47b1-c4d4-e90ec45378d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import urllib.request as request\n",
        "url = \"https://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\"\n",
        "csvfile = \"Japanese.txt\"\n",
        "\n",
        "request.urlretrieve(url,csvfile)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Japanese.txt', <http.client.HTTPMessage at 0x7f0651526e80>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwka8iOZ9Izi",
        "colab_type": "code",
        "outputId": "2d9b8b26-7909-464e-a7a5-8e7ad233042e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!mkdir data\n",
        "!wget https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/ja.text8.zip -P data/\n",
        "!unzip data/ja.text8.zip -d data/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-01 07:29:30--  https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/ja.text8.zip\n",
            "Resolving s3-ap-northeast-1.amazonaws.com (s3-ap-northeast-1.amazonaws.com)... 52.219.0.206\n",
            "Connecting to s3-ap-northeast-1.amazonaws.com (s3-ap-northeast-1.amazonaws.com)|52.219.0.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33905114 (32M) [application/zip]\n",
            "Saving to: ‘data/ja.text8.zip’\n",
            "\n",
            "ja.text8.zip        100%[===================>]  32.33M  8.25MB/s    in 3.9s    \n",
            "\n",
            "2020-05-01 07:29:35 (8.25 MB/s) - ‘data/ja.text8.zip’ saved [33905114/33905114]\n",
            "\n",
            "Archive:  data/ja.text8.zip\n",
            "  inflating: data/ja.text8           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf7X8Rim999W",
        "colab_type": "code",
        "outputId": "296b67c1-d007-481f-f174-d7d87a6f7d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = Tokenizer(num_words = 10, oov_token='<UNK>')\n",
        "texts = ['今日 は 良い 天気 だ 。']\n",
        "tokenizer.fit_on_texts(texts)\n",
        "tokenizer.word_index\n",
        "tokenizer.index_word"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: '<UNK>', 2: '今日', 3: 'は', 4: '良い', 5: '天気', 6: 'だ', 7: '。'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj6bZdfTBJWo",
        "colab_type": "code",
        "outputId": "c540bc33-a203-4b84-f6d4-6c0980bf2e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "sequence = ['猫', 'は', 'かわいい']\n",
        "pprint(skipgrams(sequence, vocabulary_size=4, window_size=1))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "([['かわいい', 1],\n",
            "  ['猫', 'は'],\n",
            "  ['は', '猫'],\n",
            "  ['かわいい', 'は'],\n",
            "  ['は', 3],\n",
            "  ['は', 2],\n",
            "  ['猫', 3],\n",
            "  ['は', 'かわいい']],\n",
            " [0, 1, 1, 1, 0, 0, 0, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfaWSpKu9wZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(filepath, encoding = 'utf-8'):\n",
        "    with open(filepath, encoding=encoding) as f:\n",
        "      return f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUF2cq-4--1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocabulary(text, num_words = None):\n",
        "    tokenizer = Tokenizer(num_words=num_words, oov_token='<UNK>')\n",
        "    tokenizer.fit_on_texts([text])\n",
        "    return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8g91WZh_vg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(text, vocab, num_words, window_size, negative_samples):\n",
        "    data = vocab.texts_to_sequences([text]).pop()\n",
        "    sampling_table = make_sampling_table(num_words)\n",
        "    couples, labels = skipgrams(data, num_words, window_size=window_size,\n",
        "                                negative_samples = negative_samples,\n",
        "                                sampling_table = sampling_table)\n",
        "    word_target, word_context = zip(*couples)\n",
        "    word_target = np.reshape(word_target, (-1, 1))\n",
        "    word_context = np.reshape(word_context, (-1, 1))\n",
        "    labels = np.asarray(labels)\n",
        "    return [word_target, word_context], labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWz7wcFGC_Fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbeddingModel:\n",
        "\n",
        "  def __init__(self, vocab_size, emb_dim = 100):\n",
        "      self.word_input = Input(shape=(1,), name = 'word_input')\n",
        "      self.word_embed = Embedding(input_dim = vocab_size,\n",
        "                                  output_dim = emb_dim,\n",
        "                                  input_length=1,\n",
        "                                  name = 'word_embedding')\n",
        "      \n",
        "      self.context_input = Input(shape=(1,), name = 'context_input')\n",
        "      self.context_embed = Embedding(input_dim = vocab_size,\n",
        "                                     output_dim = emb_dim,\n",
        "                                     input_length = 1,\n",
        "                                     name = 'context_embedding')\n",
        "      \n",
        "      self.dot = Dot(axes=2)\n",
        "      self.flatten = Flatten()\n",
        "      self.output = Dense(1, activation = 'sigmoid')\n",
        "\n",
        "\n",
        "  def build(self):\n",
        "      word_embed = self.word_embed(self.word_input)\n",
        "      context_embed = self.context_embed(self.context_input)\n",
        "      dot = self.dot([word_embed, context_embed])\n",
        "      flatten = self.flatten(dot)\n",
        "      output = self.output(flatten)\n",
        "      model = Model(inputs = [self.word_input, self.context_input],\n",
        "                    outputs=output)\n",
        "      \n",
        "      return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXfAlZM_F1y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InferenceAPI:\n",
        "\n",
        "    def __init__(self, model, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.weights = model.get_layer('word_embedding').get_weights()[0]\n",
        "\n",
        "\n",
        "    def most_similar(self, word, topn=10):\n",
        "        word_index = self.vocab.word_index.get(word, 1)\n",
        "        sim = self._cosine_similarity(word_index)\n",
        "        pairs = [(s, i) for i, s in enumerate(sim)]\n",
        "        pairs.sort(reverse = True)\n",
        "        pairs = paris[1: topn +1]\n",
        "        res = [(self.vocab.index_word[i], s) for s, i in paris]\n",
        "        return res\n",
        "\n",
        "    def similarity(self, word1, word2):\n",
        "        word_index1 = self.vocab.word_index.get(word1,1)\n",
        "        word_index2 = self.vocab.word_index.get(word2,1)\n",
        "        weight1 = self.weights[word_index1]\n",
        "        weight2 = self.weights[word_index2]\n",
        "        return cosine(weight1, weight2)\n",
        "\n",
        "\n",
        "    def _cosine_similarity(self, target_idx):\n",
        "        target_weight = self.weights[target_idx]\n",
        "        similarity = cosine_similarity(self.weights, [target_weight])\n",
        "        return similarity.flatten()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B94gQsV4TKgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    emb_dim = 50\n",
        "    epochs = 10\n",
        "    model_path = 'model.h5'\n",
        "    negative_samples = 1\n",
        "    num_words = 10000\n",
        "    window_size = 1\n",
        "\n",
        "    text = load_data(filepath = 'data/ja.text8')\n",
        "\n",
        "    vocab = build_vocabulary(text, num_words)\n",
        "\n",
        "    x,y = create_dataset(text, vocab, num_words, window_size, negative_samples)\n",
        "\n",
        "    model = EmbeddingModel(num_words, emb_dim)\n",
        "    model = model.build()\n",
        "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(patience=1),\n",
        "        ModelCheckpoint(model_path, save_best_only=True)\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        x = x,\n",
        "        y = y,\n",
        "        batch_size = 128,\n",
        "        epochs = epochs,\n",
        "        validation_split = 0.2,\n",
        "        callbacks = callbacks\n",
        "    )\n",
        " \n",
        "  \n",
        "    model = load_model(model_path)\n",
        "    api = InferenceAPI(model, vocab)\n",
        "    pprint(api.most_similar(word = '日本'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCzTxhbxuxH5",
        "colab_type": "code",
        "outputId": "e37b603f-7207-4693-fd4f-b5c25fbe4d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "if __name__ == \"__main__\" :\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "59581/59581 [==============================] - 661s 11ms/step - loss: 0.2429 - val_loss: 0.2077\n",
            "Epoch 2/10\n",
            "59581/59581 [==============================] - 666s 11ms/step - loss: 0.1875 - val_loss: 0.1890\n",
            "Epoch 3/10\n",
            "59581/59581 [==============================] - 668s 11ms/step - loss: 0.1697 - val_loss: 0.1810\n",
            "Epoch 4/10\n",
            "59581/59581 [==============================] - ETA: 0s - loss: 0.1596"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tir11PeH5f08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}